{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7236315,"sourceType":"datasetVersion","datasetId":4190515},{"sourceId":7236870,"sourceType":"datasetVersion","datasetId":4190910},{"sourceId":7252277,"sourceType":"datasetVersion","datasetId":4201991},{"sourceId":7260063,"sourceType":"datasetVersion","datasetId":4207414}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -Uqqq pip --progress-bar off\n!pip install -qqq peft==0.5.0 --progress-bar off\n!pip install -qqq bitsandbytes==0.41.1 --progress-bar off\n!pip install -qqq trl==0.7.1 --progress-bar off\n!pip install einops\n!pip install -qqq datasets==2.15.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-12T03:03:21.743143Z","iopub.execute_input":"2024-01-12T03:03:21.743453Z","iopub.status.idle":"2024-01-12T03:04:53.183464Z","shell.execute_reply.started":"2024-01-12T03:03:21.743426Z","shell.execute_reply":"2024-01-12T03:04:53.182222Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport re\nfrom pprint import pprint\n\nimport pandas as pd\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom huggingface_hub import notebook_login\nfrom peft import LoraConfig, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline\n)\nfrom trl import SFTTrainer\n\nDEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nMODEL_NAME = \"trick4kid/PhoGPT-7B5-Instruct-Patch\"","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:04:53.185650Z","iopub.execute_input":"2024-01-12T03:04:53.185980Z","iopub.status.idle":"2024-01-12T03:05:11.090715Z","shell.execute_reply.started":"2024-01-12T03:04:53.185953Z","shell.execute_reply":"2024-01-12T03:05:11.089879Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"notebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:05:11.091898Z","iopub.execute_input":"2024-01-12T03:05:11.092496Z","iopub.status.idle":"2024-01-12T03:05:11.118797Z","shell.execute_reply.started":"2024-01-12T03:05:11.092467Z","shell.execute_reply":"2024-01-12T03:05:11.117713Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b6a2234b3c40ea8c4790f700cb5963"}},"metadata":{}}]},{"cell_type":"code","source":"def create_model_and_tokenizer():\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=bnb_config,\n        trust_remote_code=True,\n        device_map=\"auto\",\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(\"vinai/PhoGPT-7B5-Instruct\")\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n\n    return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:27:16.875771Z","iopub.execute_input":"2024-01-12T03:27:16.876178Z","iopub.status.idle":"2024-01-12T03:27:16.881987Z","shell.execute_reply.started":"2024-01-12T03:27:16.876148Z","shell.execute_reply":"2024-01-12T03:27:16.881110Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model, tokenizer = create_model_and_tokenizer()\nmodel.config.use_cache = False","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:27:27.687721Z","iopub.execute_input":"2024-01-12T03:27:27.688636Z","iopub.status.idle":"2024-01-12T03:28:36.400832Z","shell.execute_reply.started":"2024-01-12T03:27:27.688603Z","shell.execute_reply":"2024-01-12T03:28:36.399995Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c8ad0f86963419a8dab4637a818fc8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99f827815c9f439e8946c7a147ea3cc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a41392378e64c27a03be317bf3a8aba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15a385bd4b2c47c7b9ac844ee9e84f44"}},"metadata":{}}]},{"cell_type":"code","source":"model.config.quantization_config.to_dict()","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:34:23.099283Z","iopub.execute_input":"2024-01-12T03:34:23.099686Z","iopub.status.idle":"2024-01-12T03:34:23.108374Z","shell.execute_reply.started":"2024-01-12T03:34:23.099656Z","shell.execute_reply":"2024-01-12T03:34:23.107239Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n 'load_in_8bit': False,\n 'load_in_4bit': True,\n 'llm_int8_threshold': 6.0,\n 'llm_int8_skip_modules': None,\n 'llm_int8_enable_fp32_cpu_offload': False,\n 'llm_int8_has_fp16_weight': False,\n 'bnb_4bit_quant_type': 'nf4',\n 'bnb_4bit_use_double_quant': False,\n 'bnb_4bit_compute_dtype': 'float16'}"},"metadata":{}}]},{"cell_type":"code","source":"lora_r = 16\nlora_alpha = 64\nlora_dropout = 0.1\nlora_target_modules = [\n    \"q_proj\",\n    \"up_proj\",\n    \"o_proj\",\n    \"k_proj\",\n    \"down_proj\",\n    \"gate_proj\",\n    \"v_proj\",\n]\n\n\npeft_config = LoraConfig(\n    r=lora_r,\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    target_modules=lora_target_modules,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset('chienpham/vnpara_train')\ndataset = dataset['train'].train_test_split(test_size = 0.2)\ndataset['validation'] = dataset['test']","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:34:34.413784Z","iopub.execute_input":"2024-01-12T03:34:34.414049Z","iopub.status.idle":"2024-01-12T03:34:35.988440Z","shell.execute_reply.started":"2024-01-12T03:34:34.414026Z","shell.execute_reply":"2024-01-12T03:34:35.987659Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def generate_training_prompt(\n    s1: str, s2:str, lbl: int):\n    lb = \"Có.\" if lbl == 1 else \"Không.\" \n    return f\"\"\"### Câu hỏi: \nHai câu sau có tương đồng về mặt ngữ nghĩa không?\nCâu 1: {s1}\nCâu 2: {s2}\nTrả lời \"Có\" hoặc \"Không\".\n\n### Trả lời:{lb}\"\"\".strip()","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:34:35.989520Z","iopub.execute_input":"2024-01-12T03:34:35.989787Z","iopub.status.idle":"2024-01-12T03:34:35.995044Z","shell.execute_reply.started":"2024-01-12T03:34:35.989764Z","shell.execute_reply":"2024-01-12T03:34:35.993967Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def generate_text(row):\n    s1 = row['sentence1']\n    s2 = row['sentence2']\n    label = row['label']\n    return {\n        \"text\": generate_training_prompt(s1, s2, label),\n    }","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:34:35.996829Z","iopub.execute_input":"2024-01-12T03:34:35.997072Z","iopub.status.idle":"2024-01-12T03:34:36.008132Z","shell.execute_reply.started":"2024-01-12T03:34:35.997049Z","shell.execute_reply":"2024-01-12T03:34:36.007375Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(generate_text(dataset['train'][0])['text'])","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:34:40.432161Z","iopub.execute_input":"2024-01-12T03:34:40.432535Z","iopub.status.idle":"2024-01-12T03:34:40.438472Z","shell.execute_reply.started":"2024-01-12T03:34:40.432504Z","shell.execute_reply":"2024-01-12T03:34:40.437377Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"### Câu hỏi: \nHai câu sau có tương đồng về mặt ngữ nghĩa không?\nCâu 1: 20 em có nghề mà chưa có việc làm nên Lê Trung Thực đã cố gắng đi khắp nơi gõ cửa nhiều cơ quan tìm việc làm cho các cháu .\nCâu 2: Nhìn 20 em có nghề mà chưa có việc làm Lê Trung Thực chạy ngược chạy xuôi gõ cửa nhiều cơ quan tìm việc làm cho các cháu .\nTrả lời \"Có\" hoặc \"Không\".\n\n### Trả lời:Có.\n","output_type":"stream"}]},{"cell_type":"code","source":"def process_dataset(data: Dataset):\n    return (\n        data.shuffle(seed=30)\n        .map(generate_text)\n        .remove_columns(\n            [\n                \"sentence1\",\n                \"sentence2\",\n                \"label\"\n            ]\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:34:44.154153Z","iopub.execute_input":"2024-01-12T03:34:44.154496Z","iopub.status.idle":"2024-01-12T03:34:44.159642Z","shell.execute_reply.started":"2024-01-12T03:34:44.154471Z","shell.execute_reply":"2024-01-12T03:34:44.158618Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"dataset[\"train\"] = process_dataset(dataset[\"train\"])\ndataset[\"validation\"] = process_dataset(dataset[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:34:47.009665Z","iopub.execute_input":"2024-01-12T03:34:47.010572Z","iopub.status.idle":"2024-01-12T03:34:47.414520Z","shell.execute_reply.started":"2024-01-12T03:34:47.010535Z","shell.execute_reply":"2024-01-12T03:34:47.413682Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1968 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24ca30bdbaa647b1bbc2c6923988455c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/493 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91fdbe48ed3c484d80bd34354a3b803d"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:34:49.469965Z","iopub.execute_input":"2024-01-12T03:34:49.470893Z","iopub.status.idle":"2024-01-12T03:34:49.476494Z","shell.execute_reply.started":"2024-01-12T03:34:49.470857Z","shell.execute_reply":"2024-01-12T03:34:49.475548Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1968\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 493\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 493\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"OUTPUT_DIR = \"experiments\"\ntraining_arguments = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=8,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=1,\n    learning_rate=1e-4,\n    fp16=True,\n    max_grad_norm=0.3,\n    num_train_epochs=2,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    warmup_ratio=0.05,\n    save_strategy=\"epoch\",\n    group_by_length=True,\n    output_dir=OUTPUT_DIR,\n    report_to=\"tensorboard\",\n    save_safetensors=True,\n    lr_scheduler_type=\"cosine\",\n    seed=42,\n)\n\nlora_r = 16\nlora_alpha = 64\nlora_dropout = 0.1\nlora_target_modules = [\n    \"q_proj\",\n    \"up_proj\",\n    \"o_proj\",\n    \"k_proj\",\n    \"down_proj\",\n    \"gate_proj\",\n    \"v_proj\",\n]\n\n\npeft_config = LoraConfig(\n    r=lora_r,\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    target_modules=lora_target_modules,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:34:52.150348Z","iopub.execute_input":"2024-01-12T03:34:52.150690Z","iopub.status.idle":"2024-01-12T03:34:52.160379Z","shell.execute_reply.started":"2024-01-12T03:34:52.150664Z","shell.execute_reply":"2024-01-12T03:34:52.159387Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=4096,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-12T03:34:55.771032Z","iopub.execute_input":"2024-01-12T03:34:55.771409Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T07:20:34.921848Z","iopub.execute_input":"2023-12-22T07:20:34.922921Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='126' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [126/246 39:32 < 38:15, 0.05 it/s, Epoch 1.02/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.621300</td>\n      <td>1.813252</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.748900</td>\n      <td>1.695111</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\nmodel_to_save.save_pretrained(\"outputs\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import get_peft_model\nlora_config = LoraConfig.from_pretrained('outputs')\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(\"chienpham/phogpt-test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}